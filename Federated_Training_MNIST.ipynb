{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e2904-ed56-484e-93d7-506333644060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.autograd.functional import jvp\n",
    "from defense import *\n",
    "import inversefed\n",
    "arch = 'ConvNet32'\n",
    "\n",
    "torch.manual_seed(50)\n",
    "\n",
    "batch_size = 8\n",
    "idx = 30\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "\n",
    "import inversefed\n",
    "setup = inversefed.utils.system_startup()\n",
    "defs = inversefed.training_strategy('conservative')\n",
    "\n",
    "loss_fn, trainloader, validloader =  inversefed.construct_dataloaders('MNIST', defs)\n",
    "dst=validloader.dataset\n",
    "\n",
    "\n",
    "train_indices = list(range(idx, idx + batch_size))\n",
    "train_subset = Subset(dst, train_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_indices = list(range(len(dst)))\n",
    "test_subset = Subset(dst, test_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7876f76-254d-4583-a717-66491f6320e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, num_channels=1):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2)  # 14x14 -> 7x7\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c282b-f5ae-4d31-8454-3d99f695d981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "split_batch=4\n",
    "test_loader=train_loader\n",
    "defense_method = lambda x,y,z,w: defense_dpsgd(x,y,z,w,clipping_threshold=1)\n",
    "# defense_method could also be defense_prune, defense_noise, defense_round\n",
    "num_epoch = 3\n",
    "lr = 1e-3\n",
    "criterion = lambda x,y:loss_fn(x,y)[0]\n",
    "\n",
    "for noise_scale in [1e-5]:\n",
    "    for num_samples in [10]: #Num samples for random sketching\n",
    "        for defense_type in ['ours']:\n",
    "            train_indices = list(range(1,4097))\n",
    "            train_subset = Subset(dst, train_indices)\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            test_indices = list(range(5001,6025))\n",
    "            test_subset = Subset(dst, test_indices)\n",
    "            test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "            print(f'\\n\\n\\n\\n___^^^___noise_scale:{noise_scale}, defense_method:{defense_type}___^^^___')\n",
    "\n",
    "            torch.manual_seed(0)\n",
    "            net=SimpleConvNet()\n",
    "            net.to(device)\n",
    "            loss = []\n",
    "            acc = []\n",
    "            norms = []\n",
    "            defense = True\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "            for _ in range(num_epoch):\n",
    "                print(f'______EPOCH {_+1}_______')\n",
    "                t=tqdm(train_loader)\n",
    "                for gt_data, gt_label in t:\n",
    "                    gt_data, gt_label = gt_data.to(device), gt_label.to(device)\n",
    "                    gt_onehot_label = label_to_onehot(gt_label, num_classes=10)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    split_size = batch_size // split_batch\n",
    "                    parts_data = torch.split(gt_data, split_size)\n",
    "                    parts_label = torch.split(gt_label, split_size)\n",
    "                    parts_onehot_label = torch.split(gt_onehot_label, split_size)\n",
    "\n",
    "                    accumulated_gradients = None\n",
    "\n",
    "                    for part_data, part_label, part_onehot_label in zip(parts_data, parts_label, parts_onehot_label):\n",
    "                        part_data.requires_grad_(True)\n",
    "                        out = net(part_data)\n",
    "                        y = criterion(out, part_label)\n",
    "                        original_dy_dx = torch.autograd.grad(y, net.parameters(), create_graph=True)\n",
    "                        l2_norms_dy_dx=None\n",
    "\n",
    "                        if defense:\n",
    "                            if defense_type != 'default':\n",
    "                                l2_norms_dy_dx = compute_l2_norm_of_gradients_new(net, part_data, part_onehot_label, criterion, num_samples=num_samples)\n",
    "                            modified_dy_dx = defense_method(original_dy_dx, l2_norms_dy_dx, noise_scale, defense_type)\n",
    "                        else:\n",
    "                            modified_dy_dx = original_dy_dx\n",
    "\n",
    "                        if accumulated_gradients is None:\n",
    "                            accumulated_gradients = [grad.clone() for grad in modified_dy_dx]\n",
    "                        else:\n",
    "                            for i in range(len(accumulated_gradients)):\n",
    "                                accumulated_gradients[i] += modified_dy_dx[i]\n",
    "\n",
    "                    for i in range(len(accumulated_gradients)):\n",
    "                        accumulated_gradients[i] /= split_batch\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for param, grad in zip(net.parameters(), accumulated_gradients):\n",
    "                            param.grad = grad\n",
    "                    optimizer.step()\n",
    "\n",
    "                    loss.append(y.item())\n",
    "                    acc.append((torch.argmax(net(gt_data), dim=1) == gt_label).float().mean().item())\n",
    "                    if defense and defense_type=='ours':\n",
    "                        normalizer=[(torch.clamp(torch.abs(l2_norms_dy_dx[i]/torch.clamp(original_dy_dx[i],min=1e-6)),min=1e-6,max=1e6))**0.5 for i in range(len(original_dy_dx))]\n",
    "                        s=mean_of_square_tensors(normalizer)**0.5\n",
    "                        norms.append([torch.mean(i/s).item() for i in normalizer])\n",
    "                    t.set_description(f\"Loss: {loss[-1]}, Acc: {acc[-1]}\")\n",
    "\n",
    "            epoch_test_loss = 0\n",
    "            epoch_test_acc = 0\n",
    "            with torch.no_grad():\n",
    "                for test_data, test_label in test_loader:\n",
    "                    test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "                    test_out = net(test_data)\n",
    "                    test_y = criterion(test_out, test_label)\n",
    "                    epoch_test_loss += test_y.item()\n",
    "                    epoch_test_acc += (torch.argmax(test_out, dim=1) == test_label).float().mean().item()\n",
    "\n",
    "            final_test_loss=epoch_test_loss / len(test_loader)\n",
    "            final_test_acc=epoch_test_acc / len(test_loader)\n",
    "            print(f\"Final Test Loss: {final_test_loss}, Final Test Acc: {final_test_acc}\")\n",
    "\n",
    "            epoch_train_loss = 0\n",
    "            epoch_train_acc = 0\n",
    "            with torch.no_grad():\n",
    "                for test_data, test_label in train_loader:\n",
    "                    test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "                    test_out = net(test_data)\n",
    "                    test_y = criterion(test_out, test_label)\n",
    "                    epoch_train_loss += test_y.item()\n",
    "                    epoch_train_acc += (torch.argmax(test_out, dim=1) == test_label).float().mean().item()\n",
    "\n",
    "            final_train_loss=epoch_train_loss / len(train_loader)\n",
    "            final_train_acc=epoch_train_acc / len(train_loader)\n",
    "            print(f\"Final Train Loss: {final_train_loss}, Final Train Acc: {final_train_acc}\")\n",
    "            \n",
    "            with open('data_train_detail.txt', 'a') as file:\n",
    "                file.write(f\"k: {noise_scale}, Defense type: {defense_type}\\nLoss: {loss}\\nAcc: {acc}\\n\\n\")\n",
    "\n",
    "            with open('data_train.txt', 'a') as file:\n",
    "                file.write(f\"Final train loss: {final_train_loss}, Final test loss: {final_test_loss}, Final train acc: {final_train_acc}, Final test acc: {final_test_acc}, k: {noise_scale}, Defense type: {defense_type}, Num_Samples: {num_samples}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bede7-e585-45d3-af5e-c3c9b7f34e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12\n",
    "plt.figure(figsize=(6,6))\n",
    "sum_range=10\n",
    "\n",
    "for _,pair in enumerate(net.named_parameters()):\n",
    "    name,param=pair\n",
    "    print(name,torch.numel(param))\n",
    "    if 'bias' not in name:\n",
    "        toplot=[x[_] for x in norms]\n",
    "        plt.plot([np.sum(toplot[max(0, i-sum_range+1):i+1])/len(toplot[max(0, i-sum_range+1):i+1]) for i in range(sum_range,len(toplot))],label=name)\n",
    "plt.legend()\n",
    "plt.savefig('MNISTnorms_weight.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
