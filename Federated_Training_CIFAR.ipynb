{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.autograd.functional import jvp\n",
    "from defense import *\n",
    "torch.manual_seed(50)\n",
    "\n",
    "batch_size = 2\n",
    "idx = 30\n",
    "\n",
    "import inversefed\n",
    "setup = inversefed.utils.system_startup()\n",
    "defs = inversefed.training_strategy('conservative')\n",
    "\n",
    "loss_fn, trainloader, validloader =  inversefed.construct_dataloaders('CIFAR10', defs)\n",
    "\n",
    "\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)\n",
    "\n",
    "dst = validloader.dataset\n",
    "\n",
    "\n",
    "train_indices = list(range(idx, idx + batch_size))\n",
    "train_subset = Subset(dst, train_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_indices = list(range(len(dst)))\n",
    "test_subset = Subset(dst, test_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inversefed\n",
    "arch = 'ConvNet32'\n",
    "\n",
    "net, _ = inversefed.construct_model(arch, num_classes=10, num_channels=3,seed=0)\n",
    "\n",
    "print('Num of Parameters Total')\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "split_batch=4\n",
    "test_loader=train_loader\n",
    "defense_method = lambda x,y,z,w: defense_dpsgd(x,y,z,w,clipping_threshold=0.05)\n",
    "defense_method = defense_prune\n",
    "num_epoch = 10\n",
    "lr = 1e-3\n",
    "\n",
    "for noise_scale in [0.1,0.2,0.3,0.4,0.5,0.6,0.7]:\n",
    "    for num_samples in [10]: #Num samples for random sketching\n",
    "        for defense_type in ['default','ours']:\n",
    "            train_indices = list(range(1,9))\n",
    "            train_subset = Subset(dst, train_indices)\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            test_indices = list(range(1,9))\n",
    "            test_subset = Subset(dst, test_indices)\n",
    "            test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "            print(f'\\n\\n\\n\\n___^^^___noise_scale:{noise_scale}, defense_method:{defense_type}___^^^___')\n",
    "\n",
    "            torch.manual_seed(0)\n",
    "            net, _ = inversefed.construct_model(arch, num_classes=10, num_channels=3, seed=0)\n",
    "            net.to(device)\n",
    "            loss = []\n",
    "            acc = []\n",
    "            norms = []\n",
    "            defense = True\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "            criterion = lambda x,y: loss_fn(x,y)[0]#torch.nn.CrossEntropyLoss()\n",
    "            for _ in range(num_epoch):\n",
    "                print(f'______EPOCH {_+1}_______')\n",
    "                t=tqdm(train_loader)\n",
    "                for gt_data, gt_label in t:\n",
    "                    gt_data, gt_label = gt_data.to(device), gt_label.to(device)\n",
    "                    gt_onehot_label = label_to_onehot(gt_label, num_classes=10)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    split_size = batch_size // split_batch\n",
    "                    parts_data = torch.split(gt_data, split_size)\n",
    "                    parts_label = torch.split(gt_label, split_size)\n",
    "                    parts_onehot_label = torch.split(gt_onehot_label, split_size)\n",
    "\n",
    "                    accumulated_gradients = None\n",
    "\n",
    "                    for part_data, part_label, part_onehot_label in zip(parts_data, parts_label, parts_onehot_label):\n",
    "                        part_data.requires_grad_(True)\n",
    "                        out = net(part_data)\n",
    "                        y = criterion(out, part_label)\n",
    "                        original_dy_dx = torch.autograd.grad(y, net.parameters(), create_graph=True)\n",
    "                        l2_norms_dy_dx=None\n",
    "\n",
    "                        if defense:\n",
    "                            if defense_type != 'default':\n",
    "                                l2_norms_dy_dx = compute_l2_norm_of_gradients_new(net, part_data, part_onehot_label, criterion, num_samples=num_samples)\n",
    "                            modified_dy_dx = defense_method(original_dy_dx, l2_norms_dy_dx, noise_scale, defense_type)\n",
    "                            # print(sum([torch.sum(i==0) for i in modified_dy_dx])/sum([torch.numel(i) for i in modified_dy_dx]))\n",
    "                        else:\n",
    "                            modified_dy_dx = original_dy_dx\n",
    "\n",
    "                        if accumulated_gradients is None:\n",
    "                            accumulated_gradients = [grad.clone() for grad in modified_dy_dx]\n",
    "                        else:\n",
    "                            for i in range(len(accumulated_gradients)):\n",
    "                                accumulated_gradients[i] += modified_dy_dx[i]\n",
    "\n",
    "                    for i in range(len(accumulated_gradients)):\n",
    "                        accumulated_gradients[i] /= split_batch\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for param, grad in zip(net.parameters(), accumulated_gradients):\n",
    "                            param.grad = grad\n",
    "                    optimizer.step()\n",
    "\n",
    "                    loss.append(y.item())\n",
    "                    acc.append((torch.argmax(net(gt_data), dim=1) == gt_label).float().mean().item())\n",
    "                    if defense and defense_type=='ours':\n",
    "                        norms.append([(torch.mean(torch.abs(l2_norms_dy_dx[i] / original_dy_dx[i])) ** 0.5).item() for i in range(len(original_dy_dx))])\n",
    "                    t.set_description(f\"Loss: {loss[-1]}, Acc: {acc[-1]}\")\n",
    "\n",
    "            epoch_test_loss = 0\n",
    "            epoch_test_acc = 0\n",
    "            with torch.no_grad():\n",
    "                for test_data, test_label in test_loader:\n",
    "                    test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "                    test_out = net(test_data)\n",
    "                    test_y = criterion(test_out, test_label)\n",
    "                    epoch_test_loss += test_y.item()\n",
    "                    epoch_test_acc += (torch.argmax(test_out, dim=1) == test_label).float().mean().item()\n",
    "\n",
    "            final_test_loss=epoch_test_loss / len(test_loader)\n",
    "            final_test_acc=epoch_test_acc / len(test_loader)\n",
    "            print(f\"Final Test Loss: {final_test_loss}, Final Test Acc: {final_test_acc}\")\n",
    "\n",
    "            epoch_train_loss = 0\n",
    "            epoch_train_acc = 0\n",
    "            with torch.no_grad():\n",
    "                for test_data, test_label in train_loader:\n",
    "                    test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "                    test_out = net(test_data)\n",
    "                    test_y = criterion(test_out, test_label)\n",
    "                    epoch_train_loss += test_y.item()\n",
    "                    epoch_train_acc += (torch.argmax(test_out, dim=1) == test_label).float().mean().item()\n",
    "\n",
    "            final_train_loss=epoch_train_loss / len(train_loader)\n",
    "            final_train_acc=epoch_train_acc / len(train_loader)\n",
    "            print(f\"Final Train Loss: {final_train_loss}, Final Train Acc: {final_train_acc}\")\n",
    "            \n",
    "            with open('data_train_detail.txt', 'a') as file:\n",
    "                file.write(f\"k: {noise_scale}, Defense type: {defense_type}\\nLoss: {loss}\\nAcc: {acc}\\n\\n\")\n",
    "\n",
    "            with open('data_train.txt', 'a') as file:\n",
    "                file.write(f\"Final train loss: {final_train_loss}, Final test loss: {final_test_loss}, Final train acc: {final_train_acc}, Final test acc: {final_test_acc}, k: {noise_scale}, Defense type: {defense_type}, Num_Samples: {num_samples}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
